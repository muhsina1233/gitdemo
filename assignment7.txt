import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

// Disable Logs
Logger.getLogger("org").setLevel(Level.OFF)
val spark = SparkSession.builder().appName("WebLog").master("local[*]").getOrCreate()

import spark.implicits._

val logs_DF = spark.read.option("header","true").csv("dbfs:/FileStore/shared_uploads/jishaunni96@gmail.com/Weblog-3.csv")
logs_DF.printSchema()

// Display top 5 rows of data
logs_DF.show(5,false)

//Souce IP/Host
val hosts = logs_DF.select(regexp_extract($"IP","""([^(\s|,)]+)""", 1).alias("host"))
hosts.show()

//Timestamp
val timestamp = logs_DF.select(regexp_extract($"Time", """\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})""",1).alias("Timestamp"))
timestamp.show()

//method
val method = logs_DF.select(regexp_extract($"URL", """(\S*\S)""", 1).alias("Method"))
method.show()

//HTTP Protocol
val method = logs_DF.select(regexp_extract($"URL", """(\S+)\s(\S+)\s*(\S*)""", 3).alias("HTTP protocol"))
method.show()

//Request URL
val url = logs_DF.select(regexp_extract($"URL", """(\S+)\s(\S+)\s*(\S*)""", 2).alias("URL"))
url.show()

//Status Code
val method = logs_DF.select(regexp_extract($"Staus", """(\S*\S)$""", 1).alias("status"))
method.show(false)

a// Merge multiple regular expressions
val log_df = logs_DF.select(regexp_extract($"IP","""([^(\s|,)]+)""", 1).alias("host"),
                           regexp_extract($"Time", """\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})""",1).alias("Timestamp"),
                           regexp_extract($"URL", """(\S*\S)""", 1).alias("Method"),
                           regexp_extract($"URL", """(\S+)\s(\S+)\s*(\S*)""", 3).alias("HTTP protocol"),
                           regexp_extract($"URL", """(\S+)\s(\S+)\s*(\S*)""", 2).alias("URL"),
                           regexp_extract($"Staus", """(\S*\S)$""", 1).alias("status"))
log_df.show()


log_df.printSchema()

b
import org.apache.spark.sql.functions.{col,when,count}
import org.apache.spark.sql.Column

def countNullCols (columns:Array[String]):Array[Column] = {
   columns.map(c => {
   count(when(col(c).isNull, c)).alias(c)
  })
}
log_df.select(countNullCols(log_df.columns): _*).show()


c
val month_map = Map("Jan" -> 1, "Feb" -> 2, "Mar" -> 3, "Apr" -> 4, "May" -> 5, "Jun" -> 6, "Jul" -> 7, "Aug" -> 8, "Sep" -> 9,
                   "Oct" -> 10, "Nov" -> 11, "Dec" -> 12)
def parse_time(s : String):String = {
  "%3$s-%2$s-%1$s %4$s:%5$s:%6$s".format(s.substring(0,2), month_map(s.substring(3,6)), s.substring(7,11), 
                                             s.substring(12,14), s.substring(15,17), s.substring(18))
}

val toTimestamp = udf[String, String](parse_time(_))

val weblog_df = log_df.select($"*", to_timestamp(toTimestamp($"Timestamp")).alias("time")).drop("Timestamp")
weblog_df.show(false)

val log_df1 = weblog_df.withColumn("day",dayofmonth($"time")).withColumn("month",month($"time")).withColumn("year",year($"time"));       
log_df1.show(5)


d
// Convert Textfile Format to Parquet File Format
log_df1.write.parquet("dbfs:/FileStore/shared_uploads/jishaunni96@gmail.com/Weblog-3/")

// Read Parquet File Format
val parquetLogs = spark.read.parquet("dbfs:/FileStore/shared_uploads/jishaunni96@gmail.com/Weblog-3/")
parquetLogs.show()

e
parquetLogs.summary().show()

f
time_df.filter($"Status" === 200).groupBy("month").count().sort(desc("count")).show(10)

g
spark.sql("select Host, count(*) as Count from weblogsTable where Status = 404 group by host order by Count Desc limit 20").show(false)

h
parquetLogs.filter($"Status" =!= 200).groupBy("URL").count().sort(desc("count")).show(15)

i
parquetLogs.filter($"Status" === 200).groupBy("URL").count().sort(desc("count")).show(10)

j
parquetLogs.createOrReplaceTempView("weblogsTable")
spark.sql("select * from weblogsTable limit 10").show()

spark.sql("select Status, count(*) as Count from weblogsTable where Status = 404 group by Status").show()

%sql
select Status, count(*) as Count from weblogsTable where Status = 404 group by Status order by Count Desc limit 20


k
spark.sql("select year, day, count(*) as Count from weblogsTable where Status = 404 group by year, day order by day").show(false)

l
spark.sql("select URL, count(*) as Count from weblogsTable where Status = 404 group by URL order by Count Desc").show(20)

m
spark.sql("select distinct(URL) from weblogsTable where Status = 404").show(false)